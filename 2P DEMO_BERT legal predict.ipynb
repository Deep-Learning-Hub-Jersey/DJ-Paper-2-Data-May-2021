{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carry out Q&A on pdf documents\n",
    "This code is designed to carry out question answering using pdf files. It can equally be used with text documents, just by loading in the text.\n",
    "\n",
    "The notebook itself does the following:\n",
    "- reads in the pdf file identified as 'filename'\n",
    "- splits the document into sentences, using the nltk library, as the maximum token length is 512\n",
    "- attempts to answer the question against each sentence, while recording the highest start and end probabilities for each sentence\n",
    "- presents the answer which has the highest probability start and end token for all sentences\n",
    "\n",
    "The model uses a pre-trained and fine tuned version of lert large, availabile from the huggingface transformers libraries. The 'bert-large-uncased-whole-word-masking-finetuned-squad' modelis re-trained using masked language modelling, and next sentence prediction. It is further fine tuned using the Stanford SQuAD dataset, which contains near to 100,000 questions and answers.\n",
    "\n",
    "The model can be further fine tuned using your own dataset through the 2F BERT DEMO BERT_LARGE FT using csv files.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lobraries\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import nltk\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model we will use\n",
    "model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "# Loads the pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loads the fine tuned model for Question Answering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set document to be loaded as filename\n",
    "filename = '2D DEMO_VitalibisInc_20180316_8-K_EX-10.2_11100168_EX-10.2_Hosting Agreement.pdf'\n",
    "# Use pdfminer to extract text from pdf\n",
    "doc = extract_text(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove characters not needed to predict\n",
    "book = doc.replace(\"\\n\" , \"\")\n",
    "book = book.replace(\"\\x0c\", \"\")\n",
    "book = book.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only required to download punctuation from NLTK once\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise document into sentences\n",
    "sent_corpus = nltk.sent_tokenize(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, sent_corpus):\n",
    "    max_prob = -10.0\n",
    "    \n",
    "    # loop through sentences\n",
    "    for sent in sent_corpus:\n",
    "        \n",
    "        # Convert text to string\n",
    "        text = str(sent)\n",
    "        \n",
    "        # Tokenise the question and text\n",
    "        inputs = tokenizer(question, text, add_special_tokens=True, max_length=512, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "        text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        # Run the tokenised text through the pre-trained auto model for  question answering, and store outputs\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Get start and end scores for each sentence from the model output\n",
    "        answer_start_scores = outputs.start_logits\n",
    "        answer_end_scores = outputs.end_logits\n",
    "\n",
    "        # Get location of maximum start score\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1 \n",
    "        \n",
    "        # Get the maximum start and end probabilities\n",
    "        max_prob_start = torch.max(answer_start_scores)\n",
    "        max_prob_end = torch.max(answer_end_scores)\n",
    "        \n",
    "        # Sum the maximum start and end probabilities\n",
    "        max_prob_startend = max_prob_start + max_prob_end\n",
    "        \n",
    "        # Check of score of prediction for sentence is higher than previously recorded\n",
    "        if max_prob_startend > max_prob:\n",
    "            max_prob = max_prob_startend\n",
    "            \n",
    "            # Convert answer tokens to string\n",
    "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "            # Store context where the answer was derived from as text answer\n",
    "            text_answer = text\n",
    "            \n",
    "    print('BERT Answer:\\n------------\\n', answer, '\\n\\nSentence:\\n---------\\n', text_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer('When is the agreement made?', sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer('Which two parties is the agreement between?', sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer('Who is the licensee?', sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer(\"What is the address of vitalibis inc\", sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer(\"What are the services provided?\", sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer(\"Are there any Additional Services?\", sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer(\"How much notice do the parties have to give?\", sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer(\"How long is the agreement for?\", sent_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
