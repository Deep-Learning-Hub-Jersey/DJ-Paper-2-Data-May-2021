{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper 2 Data Workflow for Data Extraction - CUADv1 - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources of information, code and discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. The foundation workflow is from Hugging Face's Token Classification example hosted on Colab [here][1]\n",
    "2. The models are base models, each trained using a downstream token clasification task, example [here][2]\n",
    "\n",
    "[1]: https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "[2]: https://huggingface.co/roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, random, json, string, csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification, PreTrainedModel, RobertaTokenizerFast\n",
    "\n",
    "from datasets import load_dataset, ClassLabel, Sequence \n",
    "\n",
    "import fitz # pip install PyMuPDF - PDF reader/parser\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Resolve any conflicting libraries\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face model references for Transformer library\n",
    "models = dict(\n",
    "    ROBERTA = \"roberta-base\", # Use for efficiency\n",
    "    DEBERTA_V2_XL = \"microsoft/deberta-v2-xlarge\") # Use for accuracy\n",
    "\n",
    "# RANDOM SEED FOR REPRODUCIBILITY\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# BATCH SIZE\n",
    "# IDEALLY USE SAME BATCH SIZE FOR INFERENCE AS WAS USED FOR TRAINING\n",
    "BATCH_SIZES = 2\n",
    "\n",
    "# WHICH PRE-TRAINED TRANSFORMER TO FINE-TUNE?\n",
    "MODEL_CHECKPOINT = models['ROBERTA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: File and dataset handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_CLASS_LABELS = \"feature_class_labels.json\"\n",
    "TEMP_MODEL_OUTPUT_DIR = 'temp_model_output_dir'\n",
    "SAVED_MODEL = f\"p2d-NER-Fine-Tune-Transformer-Final-{MODEL_CHECKPOINT}\" # Change for notebook version\n",
    "TEST_FILE_PATH = \"./Test_Docs/\"\n",
    "TEST_DATA_FILE = 'test_data_file.json'\n",
    "CSV_DATA_FILE = 'legal_agreement_data_file.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 5 legal agreements from ./Test_Docs/ folder:  ['1P 04_24_1998-WFS.PDF', '5P 2020-12-15 H665 OOFFS_657.pdf', '3P 06_11_2020-EX-10.1-JVA.PDF', '2P 05_04_2020-EX-10.3.PDF', '4P 060427_WELLSFARGO_MBS_TRUST_YEA.PDF']\n"
     ]
    }
   ],
   "source": [
    "# Walk through PDF files and create a dataframe with the names of the files, sorted alpha/num\n",
    "pdf_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(TEST_FILE_PATH):\n",
    "    pdf_files.extend(filenames)\n",
    "# Remove any hidden files lurking in the directory\n",
    "for i, f in enumerate(pdf_files):\n",
    "    if f.startswith(\".\"):\n",
    "        pdf_files.pop(i)\n",
    "print(f\"Uploaded {len(pdf_files)} legal agreements from {TEST_FILE_PATH} folder: \", pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function for standard PDF parsing workflow\n",
    "def pre_process_doc_common(text):\n",
    "    text = text.replace(\"\\n\", \" \")  # Simple replacement for \"\\n\"   \n",
    "    text = text.replace(\"\\xa0\", \" \")  # Simple replacement for \"\\xa0\"\n",
    "    text = text.replace(\"\\x0c\", \" \")  # Simple replacement for \"\\x0c\"\n",
    "    \n",
    "    regex = \"\\ \\.\\ \"\n",
    "    subst = \".\"\n",
    "    text = re.sub(regex, subst, text, 0)  # Get rid of multiple dots\n",
    "        \n",
    "    regex = \"_\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)  # Get rid of underscores\n",
    "       \n",
    "    regex = \"--+\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)   # Get rid of multiple dashes\n",
    "        \n",
    "    regex = \"\\*+\"\n",
    "    subst = \"*\"\n",
    "    text = re.sub(regex, subst, text, 0)  # Get rid of multiple stars\n",
    "        \n",
    "    regex = \"\\ +\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)  # Get rid of multiple whitespace\n",
    "    \n",
    "    text = text.strip()  #Strip leading and trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take in the file list, read each file, clean the text and return all agreements in a list\n",
    "def text_data(test_dir, pdf_files, print_text=False, clean_text=True, max_len=3000):\n",
    "    text_list = []\n",
    "    for filename in tqdm(pdf_files):\n",
    "        agreement = fitz.open(test_dir+filename)\n",
    "        full_text = \"\"\n",
    "        for page in agreement:\n",
    "            full_text += page.getText('text')#+\"\\n\"\n",
    "        if print_text:\n",
    "            print(\"Text before cleaning: \\n\", full_text)\n",
    "\n",
    "        # Run text through cleansing function\n",
    "        if clean_text:\n",
    "            full_text = pre_process_doc_common(full_text)\n",
    "        short_text = full_text[:max_len]\n",
    "        len_text = len(short_text)\n",
    "\n",
    "        if print_text:\n",
    "            print(\"Text after cleaning: \\n\", short_text)\n",
    "\n",
    "        text_list.append([filename, full_text, short_text, len_text])\n",
    "        \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 52.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run reading and cleaning functions on the list of PDF files in the testing folder\n",
    "# Use a max_length which is expected to capture the rich text information at the beginning of the document\n",
    "test_dir = TEST_FILE_PATH\n",
    "data = text_data(test_dir, pdf_files, print_text=False, clean_text=True, max_len=1000)\n",
    "\n",
    "# Create dataframe with text\n",
    "columns = ['File_Name','Full_Text', 'Short_Text', 'Length_Of_Short_Text']\n",
    "text_df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Full_Text</th>\n",
       "      <th>Short_Text</th>\n",
       "      <th>Length_Of_Short_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1P 04_24_1998-WFS.PDF</td>\n",
       "      <td>1 EXHIBIT 10.14 OUTSOURCING AGREEMENT This Out...</td>\n",
       "      <td>1 EXHIBIT 10.14 OUTSOURCING AGREEMENT This Out...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5P 2020-12-15 H665 OOFFS_657.pdf</td>\n",
       "      <td>DATED 4 DECEMBER 2020 INVESTOR LIMITED and INV...</td>\n",
       "      <td>DATED 4 DECEMBER 2020 INVESTOR LIMITED and INV...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3P 06_11_2020-EX-10.1-JVA.PDF</td>\n",
       "      <td>Exhibit 10.1 JOINT VENTURE AGREEMENT THIS JOIN...</td>\n",
       "      <td>Exhibit 10.1 JOINT VENTURE AGREEMENT THIS JOIN...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2P 05_04_2020-EX-10.3.PDF</td>\n",
       "      <td>Ex 10.3 SERVICING AGREEMENT between CURO RECEI...</td>\n",
       "      <td>Ex 10.3 SERVICING AGREEMENT between CURO RECEI...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4P 060427_WELLSFARGO_MBS_TRUST_YEA.PDF</td>\n",
       "      <td>EXHIBIT 10.3 Yield Maintenance Agreement [LOGO...</td>\n",
       "      <td>EXHIBIT 10.3 Yield Maintenance Agreement [LOGO...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                File_Name  \\\n",
       "0                   1P 04_24_1998-WFS.PDF   \n",
       "1        5P 2020-12-15 H665 OOFFS_657.pdf   \n",
       "2           3P 06_11_2020-EX-10.1-JVA.PDF   \n",
       "3               2P 05_04_2020-EX-10.3.PDF   \n",
       "4  4P 060427_WELLSFARGO_MBS_TRUST_YEA.PDF   \n",
       "\n",
       "                                           Full_Text  \\\n",
       "0  1 EXHIBIT 10.14 OUTSOURCING AGREEMENT This Out...   \n",
       "1  DATED 4 DECEMBER 2020 INVESTOR LIMITED and INV...   \n",
       "2  Exhibit 10.1 JOINT VENTURE AGREEMENT THIS JOIN...   \n",
       "3  Ex 10.3 SERVICING AGREEMENT between CURO RECEI...   \n",
       "4  EXHIBIT 10.3 Yield Maintenance Agreement [LOGO...   \n",
       "\n",
       "                                          Short_Text  Length_Of_Short_Text  \n",
       "0  1 EXHIBIT 10.14 OUTSOURCING AGREEMENT This Out...                  1000  \n",
       "1  DATED 4 DECEMBER 2020 INVESTOR LIMITED and INV...                  1000  \n",
       "2  Exhibit 10.1 JOINT VENTURE AGREEMENT THIS JOIN...                  1000  \n",
       "3  Ex 10.3 SERVICING AGREEMENT between CURO RECEI...                  1000  \n",
       "4  EXHIBIT 10.3 Yield Maintenance Agreement [LOGO...                  1000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at the unstructured data captured so far\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 EXHIBIT 10.14 OUTSOURCING AGREEMENT This Outsourcing Agreement (the \"Agreement\") is made and entered into as of January 1, 1998, by and between Sykes HealthPlan Services, Inc., a Florida corporation (\"SHPS\"), and HealthPlan Services, Inc., a Florida corporation (\"HPS\"). BACKGROUND HPS (or one of its affiliates other than SHPS) provides certain administrative services and Care Management Services (as defined below) to clients (\"Clients\") pursuant to the terms of agreements with such Clients (the \"Client Agreements\") as of January 1, 1998. HPS desires that SHPS provide, and SHPS is willing to provide, the Care Management Services to the Clients on behalf of HPS in accordance with the terms and conditions of this Agreement. Accordingly, in consideration of the mutual covenants and agreements set forth below, the parties agree as follows: TERMS 1. SERVICES PROVIDEDÕæ TERM AND TERMINATION 1.1 AGREEMENT TO OUTSOURCE CARE MANAGEMENT SERVICES. HPS agrees to outsource to SHPS, and hereby appoints SHPS as the exclusive provider of, Care Management Services to the Clients, subject to the terms and conditions set forth in this Agreement. SHPS shall provide the Care Management Services directly to the Clients in accordance with the terms of the Client Agreements. \"Care Management Services\" means the business of providing utilization review (which includes, but is not limited to, pre-admission certification, prior authorization, prospective length of stay approvals, second opinions, concurrent review and discharge planning), catastrophic medical case management, disease management and demand management (24 hours per day, 7 days per week) services to benefits payors and health providers, in all cases in accordance with the terms of the applicable Client Agreement. 1.2 TERM. The term of this Agreement will commence on January 1, 1998 (the \"Effective Date\") and will end on December 31, 1998. Unless either party gives the other at least ninety days\\' prior written notice that it has elected not to extend the term of this Agreement beyond December 31, 1998, the term of this Agreement will be automatically extended until December 31, 1999. Thereafter this Agreement will automatically be renewed for successive additional periods of one year, unless either party gives notice of cancellation on or before October 1 of any such year. 1.3 TERMINATION FOR CAUSE. In the event that either party materially or repeatedly defaults in the performance of any of its duties or obligations hereunder and does not substantially cure such default within thirty days after being given written notice specifying the default, or, with respect to those defaults which cannot reasonably be cured within thirty days, if the defaulting party fails to proceed promptly after being given such notice to commence curing the default and thereafter to reasonably proceed to cure the same, then the party not in default 2 may, by giving written notice to the defaulting party, terminate this Agreement as of a date specified in such notice of termination. 2. PAYMENTS 2.1 FEES FOR CURRENT HPS CLIENTS. For each month during the term of this Agreement, HPS will pay to SHPS an amount equal to (i) eighty-two and one-half percent (82.5%) of the first $500,000 of Care Management Revenues (as defined below) during such month plus (ii) eighty percent (80%) of Care Management Revenues during such month in excess of $500,000. HPS shall pay such amount to SHPS within fifteen days following the end of the applicable month. At the time of payment HPS shall submit to SHPS a schedule for the month of payment setting forth the calculation of fees payable under this Section 2.1 and Care Management Revenues by Client. 2.2 CALCULATION OF CARE MANAGEMENT REVENUES. \"Care Management Revenues\" means, with respect to any month during the term, the revenues collected by HPS from Clients (or new Clients, as applicable) for the Care Management Services. Monthly revenues for Care Management Services shall be calculated based on a per employee per month fee equal to: (i) the amount (as of the date of this Agreement) set forth in the applicable Client Agreement (including hourly medical case management fees)Õæ or (ii) if the Client Agreement does not include a per employee per month fee for Care Management Services, $2.00 (this amount shall apply to all individual and small group business). Prospectively, for new Clients, SHPS and HPS shall agree to the rate HPS will offer to such new Clients (including hourly medical case management fees). 2.3 ALLOCATION OF COSTS. SHPS shall pay to HPS its allocable portion (which portion shall approximate HPS\\' direct costs chargeable to the business function) of depreciation, information system services, rent and utilities for the use by SHPS of HPS facilities in connection with its delivery of Care Management Services to the Clients. SHPS shall also reimburse HPS for direct costs for postage and telecommunications incurred by HPS in connection with such use by SHPS of HPS facilities. For convenience, the parties acknowledge that HPS will deduct amounts owed by SHPS under this Section 2.3 from the fees described in Section 2.1 and reflect such deductions in the schedule prepared by HPS. 2.4 NEW CLIENTS. In the event that HPS (or one of its affiliates other than SHPS) enters into an agreement to provide Care Management Services with a client which is not a Client as of the date of this Agreement (a \"New Client\"), SHPS shall provide such Care Management Services to the New Client in accordance with the terms of this Agreement. HPS will pay to SHPS all Care Management Revenues collected from such New Client, and SHPS will pay a commission to HPS equal to five percent (5%) of such amount received by SHPS from HPS pursuant to this Section 2.4. HPS shall pay such amount to SHPS within fifteen days following the end of each month. At the time of payment HPS shall submit to SHPS a schedule for the month of payment setting forth the calculation of fees payable under this Section 2.4 and Care 2 3 Management Revenues by New Client. For convenience, the parties acknowledge that HPS will deduct amounts owed by SHPS under this Section 2.4 from the amounts owed by HPS under this Section 2.4 and reflect such deductions in the schedule prepared by HPS. 2.5 REPORTSÕæ AUDIT RIGHTS. For the purpose of determining the fees payable to SHPS under this Agreement, HPS shall preserve adequate records of Care Management Revenues by Client. SHPS shall have the right, upon reasonable prior written notice, to examine, copy and audit such records. Such audit shall be conducted at the location where such records are maintained and shall be at the expense of SHPS. Notwithstanding the foregoing, should any audit reveal that additional payments to SHPS are due which exceed five percent (5%) of the amount paid to SHPS for the period under audit, HPS shall pay SHPS on demand for the cost of such audit. 3. INDEMNIFICATION. Each party agrees to defend, save and hold harmless the other from and against all suits and claims that may be based on any injury to any person (including death) or to the property of any person or entity arising out of the operations of the indemnifying party or any willful act, negligence or omission of any of the indemnifying party\\'s agents, servants or employees, provided that the indemnified party shall give notice promptly in writing of any suit or claim to the other party and that the indemnified party and its agents, servants and employees shall cooperate fully with the indemnifying party and its counsel. The indemnifying party shall, at its own cost and expense, pay all charges of attorneys and all costs and other expenses arising therefrom or incurred in connection therewith, provided that it retains the right, at its own expense, to handle any action hereunder by employing its own counsel. 4. MISCELLANEOUS 4.1 CONFIDENTIALITY. SHPS and HPS each agree that all information communicated to it by the other will be held in strict confidence and will be used only for purposes of this Agreement, and that no such information will be disclosed by the recipient party, its agents or employees without the prior written consent of the other party. 4.2 BINDING NATURE AND ASSIGNMENT. This Agreement shall be binding on the parties and their respective successors and assigns, but neither party may, or shall have the power to, assign this Agreement without the prior written consent of the other, which consent shall not be unreasonably withheld. 4.3 NOTICES. Wherever under this Agreement one party is required or permitted to give notice to the other, such notice shall be deemed given when delivered in hand, or when mailed by overnight delivery or United States mail, registered or certified, return receipt requested, postage prepaid, and addressed as follows: 3 4 In the case of SHPS: Sykes HealthPlan Services, Inc. 11405 Bluegrass Parkway Louisville, Kentucky 40299 Attention: David E. Garner, President In the case of HPS: HealthPlan Services Corporation 3501 Frontage Road Tampa, Florida 33607 Attention: Philip S. Dingle, Chief Counsel 4.4 COUNTERPARTS. This Agreement may be executed in several counterparts, all of which taken together shall constitute the single agreement between the parties. 4.5 HEADINGS. The section headings used in this Agreement are for reference and convenience only and shall not enter into the interpretation of this Agreement. 4.6 RELATIONSHIP OF PARTIES. SHPS shall be and remain an independent contractor with respect to the performance of its obligations under this Agreement. Nothing contained in this Agreement shall be deemed to constitute either of the parties a joint venturer or partner of the other. 4.7 APPROVALS AND SIMILAR ACTIONS. Where agreement, approval, acceptance, consent, or similar action by either party is required by any provision of this Agreement, such action shall not be unreasonably delayed or withheld. 4.8 SEVERABILITY. If any provision of this Agreement is declared or found to be illegal, unenforceable, or void, then both parties shall be relieved of all obligations arising under such provision, but only to the extent that such provision is illegal, unenforceable, or void. 4.9 WAIVER. No delay or omission by either party to exercise any right or power in this Agreement shall impair such right or power or be construed to be a waiver of such right or power. A waiver by either of the parties shall not be construed to be a waiver of any succeeding breach or of any other covenant contained in this Agreement. 4.10 AMENDMENTS. No amendment, change, waiver, or discharge of this Agreement shall be valid unless in writing and signed by an authorized representative of the party against which such amendment, change, waiver, or discharge is sought to be enforced. 4 5 4.11 ENTIRE AGREEMENT. This Agreement constitutes the entire agreement between the parties with respect to the subject matter of this Agreement and there are no representations, understandings or agreements relating to this Agreement which are not fully expressed in this Agreement. 4.12 GOVERNING LAW. This Agreement shall be governed by and construed in accordance with the laws, other than choice of law rules, of the state of Florida. IN WITNESS WHEREOF, SHPS and HPS each caused this Agreement to be signed and delivered by its duly authorized officer, all as of the date first set forth above. SYKES HEALTHPLAN SERVICES, INC. HEALTHPLAN SERVICES, INC. By: By: Name: Name: Title: Title: 5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does an agreement look like?\n",
    "text_df['Full_Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Tokenization and feature labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-720301c73799af67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/phil/.cache/huggingface/datasets/json/default-720301c73799af67/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/phil/.cache/huggingface/datasets/json/default-720301c73799af67/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['split_tokens', 'dummy_ner_tags'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# We tokenize each agreement prior to bringing into the transformer model\n",
    "# Create tokens using spaCy\n",
    "nlp = English()\n",
    "text_df['tokens'] = text_df['Short_Text'].apply(lambda x: nlp(x))\n",
    "\n",
    "# Split tokens into a list ready for CSV\n",
    "text_df['split_tokens'] = text_df['tokens'].apply(lambda x: [tok.text for tok in x])\n",
    "\n",
    "# Create dummy NER tags for alignment purposes (a bit lazy, but convinient)\n",
    "text_df['dummy_ner_tags'] = text_df['tokens'].apply(lambda x: [0 for tok in x])\n",
    "\n",
    "# Serialise the data to JSON for archive\n",
    "export_columns = ['split_tokens', 'dummy_ner_tags']\n",
    "export_df = text_df[export_columns]\n",
    "export_df.to_json(TEST_DATA_FILE, orient=\"table\", index=False)\n",
    "text_df = text_df.drop(['dummy_ner_tags'], axis=1)\n",
    "\n",
    "# Re-import the serialized JSON data and create a dataset in the format needed for the transformer\n",
    "data_files = TEST_DATA_FILE\n",
    "datasets = load_dataset('json', data_files=data_files, field='data')\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 B-AGMT_DATE\n",
      "1 B-DOC_NAME\n",
      "2 B-PARTY\n",
      "3 I-AGMT_DATE\n",
      "4 I-DOC_NAME\n",
      "5 I-PARTY\n",
      "6 O\n"
     ]
    }
   ],
   "source": [
    "# Open the label list created in pre-processing corresponding to the ner_tag indices\n",
    "with open(FEATURE_CLASS_LABELS, 'r') as f:\n",
    "    label_list = json.load(f)\n",
    "\n",
    "for n in range(len(label_list)):\n",
    "    print(n, label_list[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "#For RoBERTa-base, need to use RobertaTokenizerFast with add_prefix_space=True to use it with pretokenized inputs.\n",
    "\n",
    "if MODEL_CHECKPOINT == models['ROBERTA']:\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions deal with split tokens and special tokens used in each Transformer model\n",
    "def word_id_func(input_ids, print_labs=False):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    word_ids = []\n",
    "    i=0\n",
    "    spec_toks = ['[CLS]', '[SEP]', '[PAD]']\n",
    "    for t in tokens:\n",
    "        if t in spec_toks:\n",
    "            word_ids.append(-100)\n",
    "            print(t, i) if print_labs else None\n",
    "        elif t.startswith('‚ñÅ'):\n",
    "            i += 1\n",
    "            word_ids.append(i)\n",
    "            print(t, i) if print_labs else None\n",
    "        else:\n",
    "            word_ids.append(i)\n",
    "            print(t, i) if print_labs else None\n",
    "        print(\"Total:\", i) if print_labs else None\n",
    "    return word_ids\n",
    "\n",
    "def tokenize_and_align_labels(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"split_tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"dummy_ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def tokenize_and_align_labels_deberta(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"split_tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids_list = []\n",
    "    for input_ids in tokenized_inputs[\"input_ids\"]:\n",
    "        wids = word_id_func(input_ids, print_labs=False)\n",
    "        word_ids_list.append(wids)\n",
    "    \n",
    "    for i, label in enumerate(examples[\"dummy_ner_tags\"]):\n",
    "        word_ids = word_ids_list[i]\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx == -100:\n",
    "                label_ids.append(-100)\n",
    "            #We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx-1])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx-1] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804060b00bed4e98874543c8c144162a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# To apply this function on all the words and labels in our dataset,\n",
    "# we just use the map method of our dataset object we created earlier.\n",
    "\n",
    "# ü§ó Datasets warns you when it uses cached files, you can pass load_from_cache_file=False in the\n",
    "# call to map to not use the cached files and force the preprocessing to be applied again.\n",
    "if MODEL_CHECKPOINT == models['DEBERTA_V2_XL']:\n",
    "    tokenize_and_align_labels = tokenize_and_align_labels_deberta\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Predictions and Inference\n",
    "\n",
    "Now to run the trained and serialised model on the evaluation set again, NOT the data used for training.\n",
    "\n",
    "Always take care to ensure that there isn't any data leakage here, eg the same agreements, different agreements from the same set of agreement or potentially different agreements from the same parties. \n",
    "\n",
    "The objective is to ensure that the model is able to generalize well to new agreements never seen before.\n",
    "\n",
    "To match the number of predictions to the original numnber of tokens, need to use: \"label_all_tokens=False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and instantiate\n",
    "loaded_model = AutoModelForTokenClassification.from_pretrained(SAVED_MODEL)\n",
    "\n",
    "args = TrainingArguments(output_dir = TEMP_MODEL_OUTPUT_DIR,\n",
    "                         per_device_train_batch_size=BATCH_SIZES,\n",
    "                         per_device_eval_batch_size=BATCH_SIZES,\n",
    "                         seed=RANDOM_SEED\n",
    "                        )\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Note instantiation currently takes a bit of time: https://github.com/huggingface/transformers/issues/9205\n",
    "# Instantiate the predictor\n",
    "pred_trainer = Trainer(\n",
    "    loaded_model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil/anaconda3/envs/p2d/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>true_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1P 04_24_1998-WFS.PDF</td>\n",
       "      <td>[O, O, O, B-DOC_NAME, I-DOC_NAME, O, B-DOC_NAM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5P 2020-12-15 H665 OOFFS_657.pdf</td>\n",
       "      <td>[O, B-AGMT_DATE, I-AGMT_DATE, I-AGMT_DATE, B-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3P 06_11_2020-EX-10.1-JVA.PDF</td>\n",
       "      <td>[O, O, B-DOC_NAME, I-DOC_NAME, I-DOC_NAME, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2P 05_04_2020-EX-10.3.PDF</td>\n",
       "      <td>[O, O, B-DOC_NAME, I-DOC_NAME, O, B-PARTY, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4P 060427_WELLSFARGO_MBS_TRUST_YEA.PDF</td>\n",
       "      <td>[O, O, B-DOC_NAME, I-DOC_NAME, I-DOC_NAME, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                File_Name  \\\n",
       "0                   1P 04_24_1998-WFS.PDF   \n",
       "1        5P 2020-12-15 H665 OOFFS_657.pdf   \n",
       "2           3P 06_11_2020-EX-10.1-JVA.PDF   \n",
       "3               2P 05_04_2020-EX-10.3.PDF   \n",
       "4  4P 060427_WELLSFARGO_MBS_TRUST_YEA.PDF   \n",
       "\n",
       "                                    true_predictions  \n",
       "0  [O, O, O, B-DOC_NAME, I-DOC_NAME, O, B-DOC_NAM...  \n",
       "1  [O, B-AGMT_DATE, I-AGMT_DATE, I-AGMT_DATE, B-P...  \n",
       "2  [O, O, B-DOC_NAME, I-DOC_NAME, I-DOC_NAME, O, ...  \n",
       "3  [O, O, B-DOC_NAME, I-DOC_NAME, O, B-PARTY, I-P...  \n",
       "4  [O, O, B-DOC_NAME, I-DOC_NAME, I-DOC_NAME, O, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the predictions\n",
    "predictions, labels, _ = pred_trainer.predict(tokenized_datasets[\"train\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "text_df['predictions'] = list(predictions)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "text_df['true_predictions'] = true_predictions\n",
    "\n",
    "# Consolidate all the information into the DataFrame\n",
    "def data_extract(tuple_list):\n",
    "    de_list = []\n",
    "    for tup in tuple_list:\n",
    "        if tup[1] != 'O':\n",
    "            de_list.append(tup)\n",
    "    return de_list\n",
    "\n",
    "text_df['check_pred'] = list(list(zip(a,b)) for a,b in zip(text_df['split_tokens'], text_df['true_predictions']))\n",
    "text_df['data_tuples'] = text_df['check_pred'].apply(data_extract)\n",
    "\n",
    "# Have a look at the label predictions\n",
    "text_df.head()[['File_Name', 'true_predictions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>agmt_name</th>\n",
       "      <th>agmt_date</th>\n",
       "      <th>agmt_parties</th>\n",
       "      <th>Full_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1P 04_24_1998-WFS.PDF</td>\n",
       "      <td>Outsourcing Agreement</td>\n",
       "      <td>January 1 , 1998</td>\n",
       "      <td>[Sykes HealthPlan Services , Inc., HealthPlan ...</td>\n",
       "      <td>1 EXHIBIT 10.14 OUTSOURCING AGREEMENT This Out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5P 2020-12-15 H665 OOFFS_657.pdf</td>\n",
       "      <td>OPTION AGREEMENT</td>\n",
       "      <td>4 December 2020</td>\n",
       "      <td>[INVESTOR LIMITED, INVESTMENT LIMITED]</td>\n",
       "      <td>DATED 4 DECEMBER 2020 INVESTOR LIMITED and INV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3P 06_11_2020-EX-10.1-JVA.PDF</td>\n",
       "      <td>JOINT VENTURE AGREEMENT</td>\n",
       "      <td>20th day of Friday , March 2020</td>\n",
       "      <td>[BorrowMoney.com , inc, JVLS , LLC dba, Vaccin...</td>\n",
       "      <td>Exhibit 10.1 JOINT VENTURE AGREEMENT THIS JOIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2P 05_04_2020-EX-10.3.PDF</td>\n",
       "      <td>SERVICING AGREEMENT</td>\n",
       "      <td>April 8 , 2020 ,</td>\n",
       "      <td>[CURO RECEIVABLES FINANCE II , LLC, CURO MANAG...</td>\n",
       "      <td>Ex 10.3 SERVICING AGREEMENT between CURO RECEI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4P 060427_WELLSFARGO_MBS_TRUST_YEA.PDF</td>\n",
       "      <td>Yield Maintenance Agreement</td>\n",
       "      <td>27 April 2006</td>\n",
       "      <td>[Wells Fargo Bank , N.A., Wells Fargo Mortgage...</td>\n",
       "      <td>EXHIBIT 10.3 Yield Maintenance Agreement [LOGO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                File_Name                    agmt_name  \\\n",
       "0                   1P 04_24_1998-WFS.PDF        Outsourcing Agreement   \n",
       "1        5P 2020-12-15 H665 OOFFS_657.pdf             OPTION AGREEMENT   \n",
       "2           3P 06_11_2020-EX-10.1-JVA.PDF      JOINT VENTURE AGREEMENT   \n",
       "3               2P 05_04_2020-EX-10.3.PDF          SERVICING AGREEMENT   \n",
       "4  4P 060427_WELLSFARGO_MBS_TRUST_YEA.PDF  Yield Maintenance Agreement   \n",
       "\n",
       "                         agmt_date  \\\n",
       "0                 January 1 , 1998   \n",
       "1                  4 December 2020   \n",
       "2  20th day of Friday , March 2020   \n",
       "3                 April 8 , 2020 ,   \n",
       "4                    27 April 2006   \n",
       "\n",
       "                                        agmt_parties  \\\n",
       "0  [Sykes HealthPlan Services , Inc., HealthPlan ...   \n",
       "1             [INVESTOR LIMITED, INVESTMENT LIMITED]   \n",
       "2  [BorrowMoney.com , inc, JVLS , LLC dba, Vaccin...   \n",
       "3  [CURO RECEIVABLES FINANCE II , LLC, CURO MANAG...   \n",
       "4  [Wells Fargo Bank , N.A., Wells Fargo Mortgage...   \n",
       "\n",
       "                                           Full_Text  \n",
       "0  1 EXHIBIT 10.14 OUTSOURCING AGREEMENT This Out...  \n",
       "1  DATED 4 DECEMBER 2020 INVESTOR LIMITED and INV...  \n",
       "2  Exhibit 10.1 JOINT VENTURE AGREEMENT THIS JOIN...  \n",
       "3  Ex 10.3 SERVICING AGREEMENT between CURO RECEI...  \n",
       "4  EXHIBIT 10.3 Yield Maintenance Agreement [LOGO...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions to extract each important data point based on the model's labeling of each token\n",
    "\n",
    "def extract_agreement_date(tuple_list):\n",
    "    for d in tuple_list:\n",
    "        if d[1] == \"B-AGMT_DATE\":\n",
    "            temp_date=d[0]\n",
    "        elif d[1] == \"I-AGMT_DATE\":\n",
    "            temp_date = temp_date + \" \" + d[0]\n",
    "        else:\n",
    "            continue\n",
    "    return temp_date\n",
    "\n",
    "text_df['agmt_date'] = text_df['data_tuples'].apply(extract_agreement_date)\n",
    "\n",
    "def extract_agreement_name(tuple_list):\n",
    "    for n in tuple_list:\n",
    "        if n[1] == \"B-DOC_NAME\":\n",
    "            temp_name=n[0]\n",
    "        elif n[1] == \"I-DOC_NAME\":\n",
    "            temp_name = temp_name + \" \" + n[0]\n",
    "        else:\n",
    "            continue\n",
    "    return temp_name\n",
    "\n",
    "text_df['agmt_name'] = text_df['data_tuples'].apply(extract_agreement_name)\n",
    "\n",
    "def extract_agreement_parties(tuple_list):\n",
    "    data_dict = defaultdict(list)\n",
    "    for i, p in enumerate(tuple_list):\n",
    "        if p[1] == \"B-PARTY\":\n",
    "            temp_party=p[0]\n",
    "            if i == (len(tuple_list)-1):\n",
    "                data_dict[\"Parties\"].append(temp_party)\n",
    "            elif tuple_list[i+1][1] != \"I-PARTY\":\n",
    "                data_dict[\"Parties\"].append(temp_party)\n",
    "        elif p[1] == \"I-PARTY\":\n",
    "            temp_party = temp_party + \" \" + p[0]\n",
    "            if i == (len(tuple_list)-1):\n",
    "                data_dict[\"Parties\"].append(temp_party)\n",
    "            elif tuple_list[i+1][1] != \"I-PARTY\":\n",
    "                data_dict[\"Parties\"].append(temp_party)\n",
    "\n",
    "    return list(dict.fromkeys(data_dict['Parties']))\n",
    "\n",
    "text_df['agmt_parties'] = text_df['data_tuples'].apply(extract_agreement_parties)\n",
    "\n",
    "# Create a dataframe with just the information we want to keep and \n",
    "export_df = text_df[['File_Name', 'agmt_name', 'agmt_date', 'agmt_parties', 'Full_Text']].copy()\n",
    "\n",
    "# Let's have a look\n",
    "export_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: \t\t 1P 04_24_1998-WFS.PDF\n",
      "Agreement Name: \t Outsourcing Agreement\n",
      "Agreement Date: \t January 1 , 1998\n",
      "Agreement Parties:\n",
      "\t\t\t Sykes HealthPlan Services , Inc.\n",
      "\t\t\t HealthPlan Services , Inc.\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "sample=0\n",
    "print(\"File Name: \\t\\t\",export_df.iloc[sample][0])\n",
    "print(\"Agreement Name: \\t\",export_df.iloc[sample][1])\n",
    "print(\"Agreement Date: \\t\",export_df.iloc[sample][2])\n",
    "print(\"Agreement Parties:\")\n",
    "for p in export_df.iloc[sample][3]:\n",
    "    print(\"\\t\\t\\t\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV file, upload to a database table or some other structured data format, we are done.\n",
    "export_df.to_csv(CSV_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
