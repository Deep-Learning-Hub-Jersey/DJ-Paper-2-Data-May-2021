{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SL## This model enables fine tuning of the T5 model using your own .csv files\n",
    "Other than the conversion to enable .csv documents to be read, this notebook leans heavilily on Venelin Valkov's excellent tutorial on youtube, referenced in the URL: https://www.youtube.com/watch?v=r6XY80Z9eSA\n",
    "\n",
    "As a true text to text transformer, this model expects the following .csv file column headers and data:\n",
    "- 'context' - This is the text which you are trying to extract the answer from\n",
    "- 'question' - This is the question being asked\n",
    "- 'answer_text' - This is the answer to the question. NOTE, the answer does not have to be in the 'context', as this is a true language model using both the Encoder and Decoder of the Transformer\n",
    "\n",
    "Note, this model is extremely accurate when using your own data to fine tune. However in the same vein, it loses generality very quickly if you are not careful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :pad libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import(AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap this for your own fine tuned data\n",
    "df = pd.read_csv('6D DEMO master_clauses_clean_updated2503.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns not needed from df\n",
    "df = df.drop(labels=['Unnamed: 0', 'Filename'], axis = 1)\n",
    "df = df.dropna()\n",
    "df = df.rename(columns={'Question' : 'question', 'Context' : 'context', 'Answer' : 'answer_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set MODEL_NAME as pre-trained t5 model, in this case t5-base\n",
    "MODEL_NAME = 't5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        source_max_token_len: int = 1000,\n",
    "        target_max_token_len: int = 1000\n",
    "    ):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        \n",
    "        source_encoding = tokenizer(\n",
    "            data_row['question'],\n",
    "            data_row['context'],\n",
    "            max_length=self.source_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation='only_second',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        target_encoding = tokenizer(\n",
    "            data_row['answer_text'],\n",
    "            max_length=self.target_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    " \n",
    "        labels = target_encoding['input_ids']\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return dict(\n",
    "        question=data_row['question'],\n",
    "            context=data_row['context'],\n",
    "            answer_text=data_row['answer_text'],\n",
    "            input_ids=source_encoding['input_ids'].flatten(),\n",
    "            attention_mask=source_encoding['attention_mask'].flatten(),\n",
    "            labels=labels.flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size = 0.05)\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        tokenizer: T5Tokenizer,\n",
    "        batch_size: int = 8,\n",
    "        source_max_token_len: int = 1000,\n",
    "        target_max_token_len: int = 1000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.data = data\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "        \n",
    "    def setup(self):\n",
    "        self.train_dataset = QADataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "        self.test_dataset = QADataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.source_max_token_len,\n",
    "            self.target_max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers = 4\n",
    "        )\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = 1,\n",
    "            num_workers = 4\n",
    "        )       \n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size = 1,\n",
    "            num_workers = 4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "N_EPOCHS = 6\n",
    "\n",
    "data_module = QADataModule(train_df, val_df, tokenizer, batch_size = BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this one\n",
    "class QAModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels=batch['labels']\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log('train_loss', loss,  logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels=batch['labels']\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log('val_loss', loss, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels=batch['labels']\n",
    "        \n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log('test_loss', loss,  logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QAModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = 'checkpoints',\n",
    "    filename = 'best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    checkpoint_callback = checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = QAModel.load_from_checkpoint('checkpoints/best-checkpoint.ckpt')\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import(AdamW, T5ForConditionalGeneration, T5TokenizerFast as T5Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    source_encoding = tokenizer(\n",
    "        question['question'],\n",
    "        question['context'],\n",
    "        max_length = 1000, \n",
    "        padding='max_length',\n",
    "        truncation='only_second',\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    print(len(source_encoding))\n",
    "    generate_ids = trained_model.model.generate(\n",
    "        input_ids=source_encoding['input_ids'],\n",
    "        attention_mask=source_encoding['attention_mask'],\n",
    "        num_beams=1,\n",
    "        max_length=1000,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    preds = [tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for generated_id in generate_ids]\n",
    "    \n",
    "    return \"\".join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question = val_df.iloc[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_question['answer_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(sample_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
